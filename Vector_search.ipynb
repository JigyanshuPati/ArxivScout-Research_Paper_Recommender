{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3715fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -qU langchain-chroma\n",
    "!pip install -qU langchain-google-genai\n",
    "!pip install -qU langchain-community\n",
    "!pip install python-dotenv\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e2174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d882124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv(\"/Users/jigyanshupati/semantic_researc_paper/arXiv_scientific_dataset_cleaned.csv\")\n",
    "df[\"tagged_summary\"].to_csv(\"tagged_summary.txt\", sep=\"\\n\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7f8138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 100\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Read the file\n",
    "with open(\"tagged_summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Match entries inside quotes\n",
    "entries = re.findall(r'\"(.*?)\"', text, re.DOTALL)\n",
    "\n",
    "# Keep only first 100 entries\n",
    "entries = entries[:100]\n",
    "\n",
    "# Create documents\n",
    "documents = [Document(page_content=entry.strip()) for entry in entries]\n",
    "\n",
    "print(\"Total chunks:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff10888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyanshupati/semantic_researc_paper/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings generated: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: Extract text\n",
    "texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# Step 2: Load the model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Step 3: Encode sequentially\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Total embeddings generated: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa78e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:07<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 500\n",
      "Total embeddings generated: 500\n",
      "Documents and embeddings added to Chroma collection successfully.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "# -------- Step 0: Read and prepare documents --------\n",
    "with open(\"tagged_summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract text entries inside quotes\n",
    "entries = re.findall(r'\"(.*?)\"', text, re.DOTALL)\n",
    "\n",
    "# Limit to first 100 entries (optional)\n",
    "entries = entries[:500]\n",
    "\n",
    "# Create Document objects\n",
    "documents = [Document(page_content=entry.strip()) for entry in entries]\n",
    "\n",
    "# Extract plain texts from documents\n",
    "texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# -------- Step 1: Load sentence transformer model and encode --------\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total embeddings generated: {len(embeddings)}\")\n",
    "\n",
    "# -------- Step 2: Initialize ChromaDB Client safely --------\n",
    "try:\n",
    "    client\n",
    "except NameError:\n",
    "    client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# -------- Step 3: Get or create collection --------\n",
    "collection = client.get_or_create_collection(name=\"tagged_summary_collection\")\n",
    "\n",
    "# -------- Step 4: Prepare ids and metadata --------\n",
    "ids = [str(uuid.uuid4()) for _ in texts]\n",
    "metadatas = [{\"source\": \"tagged_summary\"} for _ in texts]\n",
    "\n",
    "# -------- Step 5: Add documents and embeddings to collection --------\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    embeddings=embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"Documents and embeddings added to Chroma collection successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0ff1bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['4c231385-bdaf-4635-a924-b2843d1a90b5',\n",
       "   'aeb234ad-9ed3-4eb4-b85c-a697cdb0ff4a',\n",
       "   '853c819b-2353-4a7d-b2ea-564df729fcd7',\n",
       "   '756c4fc0-92e0-4331-8c1c-4bbeb68c7558',\n",
       "   '8aa21f09-4857-4df5-995c-e9346df77ee6',\n",
       "   '540fafc1-baca-49f1-85e7-a9a1954cab81',\n",
       "   'eefa72c9-27ee-444f-8a6b-a2cca6052106',\n",
       "   '68b8e361-16ae-4ffd-8da5-792a37d7b0ab',\n",
       "   '984a5f04-062b-4d03-b118-c6eeec914d95',\n",
       "   'ed219d6f-84d6-4d03-b64e-9824251ae22c']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['cs-9308101v1 Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.',\n",
       "   'cs-9308101v1 Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.',\n",
       "   'cs-0407042v1 Solution techniques for Constraint Satisfaction and Optimisation Problems\\noften make use of backtrack search methods, exploiting variable and value\\nordering heuristics. In this paper, we propose and analyse a very simple method\\nto apply in case the value ordering heuristic produces ties: postponing the\\nbranching decision. To this end, we group together values in a tie, branch on\\nthis sub-domain, and defer the decision among them to lower levels of the\\nsearch tree. We show theoretically and experimentally that this simple\\nmodification can dramatically improve the efficiency of the search strategy.\\nAlthough in practise similar methods may have been applied already, to our\\nknowledge, no empirical or theoretical study has been proposed in the\\nliterature to identify when and to what extent this strategy should be used.',\n",
       "   \"cs-9512101v1 OPUS is a branch and bound search algorithm that enables efficient admissible\\nsearch through spaces for which the order of search operator application is not\\nsignificant. The algorithm's search efficiency is demonstrated with respect to\\nvery large machine learning search spaces. The use of admissible search is of\\npotential value to the machine learning community as it means that the exact\\nlearning biases to be employed for complex learning tasks can be precisely\\nspecified and manipulated. OPUS also has potential for application in other\\nareas of artificial intelligence, notably, truth maintenance.\",\n",
       "   \"cs-9512101v1 OPUS is a branch and bound search algorithm that enables efficient admissible\\nsearch through spaces for which the order of search operator application is not\\nsignificant. The algorithm's search efficiency is demonstrated with respect to\\nvery large machine learning search spaces. The use of admissible search is of\\npotential value to the machine learning community as it means that the exact\\nlearning biases to be employed for complex learning tasks can be precisely\\nspecified and manipulated. OPUS also has potential for application in other\\nareas of artificial intelligence, notably, truth maintenance.\",\n",
       "   \"cs-0305001v1 Search in cyclic AND/OR graphs was traditionally known to be an unsolved\\nproblem. In the recent past several important studies have been reported in\\nthis domain. In this paper, we have taken a fresh look at the problem. First, a\\nnew and comprehensive theoretical framework for cyclic AND/OR graphs has been\\npresented, which was found missing in the recent literature. Based on this\\nframework, two best-first search algorithms, S1 and S2, have been developed. S1\\ndoes uninformed search and is a simple modification of the Bottom-up algorithm\\nby Martelli and Montanari. S2 performs a heuristically guided search and\\nreplicates the modification in Bottom-up's successors, namely HS and AO*. Both\\nS1 and S2 solve the problem of searching AND/OR graphs in presence of cycles.\\nWe then present a detailed analysis for the correctness and complexity results\\nof S1 and S2, using the proposed framework. We have observed through\\nexperiments that S1 and S2 output correct results in all cases.\",\n",
       "   'cs-9712102v1 The assessment of bidirectional heuristic search has been incorrect since it\\nwas first published more than a quarter of a century ago. For quite a long\\ntime, this search strategy did not achieve the expected results, and there was\\na major misunderstanding about the reasons behind it. Although there is still\\nwide-spread belief that bidirectional heuristic search is afflicted by the\\nproblem of search frontiers passing each other, we demonstrate that this\\nconjecture is wrong. Based on this finding, we present both a new generic\\napproach to bidirectional heuristic search and a new approach to dynamically\\nimproving heuristic values that is feasible in bidirectional search only. These\\napproaches are put into perspective with both the traditional and more recently\\nproposed approaches in order to facilitate a better overall understanding.\\nEmpirical results of experiments with our new approaches show that\\nbidirectional heuristic search can be performed very efficiently and also with\\nlimited memory. These results suggest that bidirectional heuristic search\\nappears to be better for solving certain difficult problems than corresponding\\nunidirectional search. This provides some evidence for the usefulness of a\\nsearch strategy that was long neglected. In summary, we show that bidirectional\\nheuristic search is viable and consequently propose that it be reconsidered.',\n",
       "   'cs-9712102v1 The assessment of bidirectional heuristic search has been incorrect since it\\nwas first published more than a quarter of a century ago. For quite a long\\ntime, this search strategy did not achieve the expected results, and there was\\na major misunderstanding about the reasons behind it. Although there is still\\nwide-spread belief that bidirectional heuristic search is afflicted by the\\nproblem of search frontiers passing each other, we demonstrate that this\\nconjecture is wrong. Based on this finding, we present both a new generic\\napproach to bidirectional heuristic search and a new approach to dynamically\\nimproving heuristic values that is feasible in bidirectional search only. These\\napproaches are put into perspective with both the traditional and more recently\\nproposed approaches in order to facilitate a better overall understanding.\\nEmpirical results of experiments with our new approaches show that\\nbidirectional heuristic search can be performed very efficiently and also with\\nlimited memory. These results suggest that bidirectional heuristic search\\nappears to be better for solving certain difficult problems than corresponding\\nunidirectional search. This provides some evidence for the usefulness of a\\nsearch strategy that was long neglected. In summary, we show that bidirectional\\nheuristic search is viable and consequently propose that it be reconsidered.',\n",
       "   'cs-0407040v1 In this paper we present and evaluate a search strategy called Decomposition\\nBased Search (DBS) which is based on two steps: subproblem generation and\\nsubproblem solution. The generation of subproblems is done through value\\nranking and domain splitting. Subdomains are explored so as to generate,\\naccording to the heuristic chosen, promising subproblems first.\\n  We show that two well known search strategies, Limited Discrepancy Search\\n(LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First\\nwe present a tuning of DBS that visits the same search nodes as IB, but avoids\\nrestarts. Then we compare both theoretically and computationally DBS and LDS\\nusing the same heuristic. We prove that DBS has a higher probability of being\\nsuccessful than LDS on a comparable number of nodes, under realistic\\nassumptions. Experiments on a constraint satisfaction problem and an\\noptimization problem show that DBS is indeed very effective if compared to LDS.',\n",
       "   'cs-9603101v1 We introduce an algorithm for combinatorial search on quantum computers that\\nis capable of significantly concentrating amplitude into solutions for some NP\\nsearch problems, on average. This is done by exploiting the same aspects of\\nproblem structure as used by classical backtrack methods to avoid unproductive\\nsearch choices. This quantum algorithm is much more likely to find solutions\\nthan the simple direct use of quantum parallelism. Furthermore, empirical\\nevaluation on small problems shows this quantum algorithm displays the same\\nphase transition behavior, and at the same location, as seen in many previously\\nstudied classical search methods. Specifically, difficult problem instances are\\nconcentrated near the abrupt change from underconstrained to overconstrained\\nproblems.']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'},\n",
       "   {'source': 'tagged_summary'}]],\n",
       " 'distances': [[0.5642638206481934,\n",
       "   0.5642638206481934,\n",
       "   0.9319877624511719,\n",
       "   1.0513007640838623,\n",
       "   1.0513007640838623,\n",
       "   1.0715713500976562,\n",
       "   1.103090524673462,\n",
       "   1.103090524673462,\n",
       "   1.1854208707809448,\n",
       "   1.1876592636108398]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"dependency-directed backtracking in search\"\n",
    "# Step 6: Encode the query with the same model\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Step 7: Query the collection with the embedding vector\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding.tolist(),  # must be a list of lists\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "# The results dict contains 'ids', 'documents', 'metadatas', 'distances'\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0933755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                 title                 category category_code  \\\n",
       "0  cs-9308101v1  Dynamic Backtracking  Artificial Intelligence         cs.AI   \n",
       "\n",
       "  published_date updated_date             authors      first_author  \\\n",
       "0         8/1/93       8/1/93  ['M. L. Ginsberg']  'M. L. Ginsberg'   \n",
       "\n",
       "                                             summary  summary_word_count  \n",
       "0  Because of their occasional need to return to ...                  79  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"/Users/jigyanshupati/semantic_researc_paper/arXiv_scientific dataset.csv\")\n",
    "first_doc = results['documents'][0][0]\n",
    "doc_id = first_doc.split()[0].strip()\n",
    "df[df[\"id\"] == doc_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bcb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Step 1: Load SentenceTransformer model ----\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ---- Step 2: Initialize ChromaDB client and collection ----\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "collection = client.get_or_create_collection(name=\"tagged_summary_collection\")\n",
    "\n",
    "# ---- Step 3: Define CSV path ----\n",
    "csv_path = \"/Users/jigyanshupati/semantic_researc_paper/arXiv_scientific dataset.csv\"\n",
    "\n",
    "# ---- Step 4: Define search function ----\n",
    "def get_recommendations(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search and return top-k results from the CSV.\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=k\n",
    "    )\n",
    "\n",
    "    # Extract document IDs from the result\n",
    "    doc_ids = [doc.split()[0].strip() for doc in results['documents'][0]]\n",
    "\n",
    "    # Load the CSV dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Filter by document ID\n",
    "    matched_df = df[df[\"id\"].isin(doc_ids)]\n",
    "\n",
    "    return matched_df\n",
    "\n",
    "# ---- Step 5: Gradio UI ----\n",
    "def search_interface(query):\n",
    "    return get_recommendations(query, k=5)\n",
    "\n",
    "gr.Interface(\n",
    "    fn=search_interface,\n",
    "    inputs=gr.Textbox(label=\"Search Query\"),\n",
    "    outputs=gr.Dataframe(label=\"Top Matches\"),\n",
    "    title=\"Semantic Paper Recommender\",\n",
    "    description=\"Enter a query to retrieve the most relevant research papers.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c51040c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>cs-9512101v1</td>\n",
       "      <td>OPUS: An Efficient Admissible Algorithm for Un...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>12/1/95</td>\n",
       "      <td>12/1/95</td>\n",
       "      <td>['G. I. Webb']</td>\n",
       "      <td>'G. I. Webb'</td>\n",
       "      <td>OPUS is a branch and bound search algorithm th...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>cs-9603101v1</td>\n",
       "      <td>Quantum Computing and Phase Transitions in Com...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>3/1/96</td>\n",
       "      <td>3/1/96</td>\n",
       "      <td>['T. Hogg']</td>\n",
       "      <td>'T. Hogg'</td>\n",
       "      <td>We introduce an algorithm for combinatorial se...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>cs-9712102v1</td>\n",
       "      <td>Bidirectional Heuristic Search Reconsidered</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>12/1/97</td>\n",
       "      <td>12/1/97</td>\n",
       "      <td>['H. Kaindl', 'G. Kainz']</td>\n",
       "      <td>'H. Kaindl'</td>\n",
       "      <td>The assessment of bidirectional heuristic sear...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>cs-0305001v1</td>\n",
       "      <td>A Framework for Searching AND/OR Graphs with C...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>5/1/03</td>\n",
       "      <td>5/1/03</td>\n",
       "      <td>['Ambuj Mahanti', 'Supriyo Ghose', 'Samir K. S...</td>\n",
       "      <td>'Ambuj Mahanti'</td>\n",
       "      <td>Search in cyclic AND/OR graphs was traditional...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>cs-0407040v1</td>\n",
       "      <td>Decomposition Based Search - A theoretical and...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>7/16/04</td>\n",
       "      <td>7/16/04</td>\n",
       "      <td>['W. J. van Hoeve', 'M. Milano']</td>\n",
       "      <td>'W. J. van Hoeve'</td>\n",
       "      <td>In this paper we present and evaluate a search...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>cs-0407042v1</td>\n",
       "      <td>Postponing Branching Decisions</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>7/16/04</td>\n",
       "      <td>7/16/04</td>\n",
       "      <td>['Willem Jan van Hoeve', 'Michela Milano']</td>\n",
       "      <td>'Willem Jan van Hoeve'</td>\n",
       "      <td>Solution techniques for Constraint Satisfactio...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              title  \\\n",
       "0    cs-9308101v1                               Dynamic Backtracking   \n",
       "39   cs-9512101v1  OPUS: An Efficient Admissible Algorithm for Un...   \n",
       "49   cs-9603101v1  Quantum Computing and Phase Transitions in Com...   \n",
       "91   cs-9712102v1        Bidirectional Heuristic Search Reconsidered   \n",
       "207  cs-0305001v1  A Framework for Searching AND/OR Graphs with C...   \n",
       "276  cs-0407040v1  Decomposition Based Search - A theoretical and...   \n",
       "277  cs-0407042v1                     Postponing Branching Decisions   \n",
       "\n",
       "                    category category_code published_date updated_date  \\\n",
       "0    Artificial Intelligence         cs.AI         8/1/93       8/1/93   \n",
       "39   Artificial Intelligence         cs.AI        12/1/95      12/1/95   \n",
       "49   Artificial Intelligence         cs.AI         3/1/96       3/1/96   \n",
       "91   Artificial Intelligence         cs.AI        12/1/97      12/1/97   \n",
       "207  Artificial Intelligence         cs.AI         5/1/03       5/1/03   \n",
       "276  Artificial Intelligence         cs.AI        7/16/04      7/16/04   \n",
       "277  Artificial Intelligence         cs.AI        7/16/04      7/16/04   \n",
       "\n",
       "                                               authors  \\\n",
       "0                                   ['M. L. Ginsberg']   \n",
       "39                                      ['G. I. Webb']   \n",
       "49                                         ['T. Hogg']   \n",
       "91                           ['H. Kaindl', 'G. Kainz']   \n",
       "207  ['Ambuj Mahanti', 'Supriyo Ghose', 'Samir K. S...   \n",
       "276                   ['W. J. van Hoeve', 'M. Milano']   \n",
       "277         ['Willem Jan van Hoeve', 'Michela Milano']   \n",
       "\n",
       "               first_author  \\\n",
       "0          'M. L. Ginsberg'   \n",
       "39             'G. I. Webb'   \n",
       "49                'T. Hogg'   \n",
       "91              'H. Kaindl'   \n",
       "207         'Ambuj Mahanti'   \n",
       "276       'W. J. van Hoeve'   \n",
       "277  'Willem Jan van Hoeve'   \n",
       "\n",
       "                                               summary  summary_word_count  \n",
       "0    Because of their occasional need to return to ...                  79  \n",
       "39   OPUS is a branch and bound search algorithm th...                  91  \n",
       "49   We introduce an algorithm for combinatorial se...                 111  \n",
       "91   The assessment of bidirectional heuristic sear...                 204  \n",
       "207  Search in cyclic AND/OR graphs was traditional...                 158  \n",
       "276  In this paper we present and evaluate a search...                 152  \n",
       "277  Solution techniques for Constraint Satisfactio...                 127  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(query = \"dependency-directed backtracking in search\", k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365e704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
